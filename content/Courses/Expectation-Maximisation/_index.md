---
title: Mixture models and Expectation-Maximisation algorithm
linkTitle: Mixture models and EM algorithm
summary: An overview of how the EM algorithm works with some practical implementation on the Gaussian Mixture Model.
date: '2021-11-04'
type: book
math: true
---

## Context
  Clustering is a one of the most ancient unsupervised tasks of ML aiming at identifying a partition of a given dataset into multiple groups called ``clusters''. Figure \ref{fig:clustering} illustrates this objective in a 2D case where the aim is to go from the left panel to the right one, by attributing a class to each input datapoint. The common ground to many methods that have been proposed these past decades is that they all embed, in some ways, a measure of the "similarity" between datapoints living inside the same cluster. The simplest way of thinking this is a measure based on the Euclidean distance leading to representations in which "close" datapoints more likely fall into the same cluster while distant ones should reside in different clusters. The prototypical method to partition a $D$-dimensional dataset into $K$ clusters is the K-means algorithm \citep{Macqueen1967}.
  Taking back the general formulation of optimisation problems of Sect \ref{subsect:optimisation}, we can write the K-means goal as finding the set of points minimising the sum of squared distances between datapoints and cluster centres. We end up in a setting seeking to find the set of points ${\mu} = \{{\mu}_1, \ldots, {\mu}_k\}\tran$ minimising
    $$
    \sum_{i=1}^N \operatorname*{min}_{k \in \{1, \ldots, K\}} \lVert {x}_i - {\mu}_k \rVert^2_2.
    $$
    The K-means algorithm solves this optimisation problem in a simple way. Starting from a set of initial positions ${\mu}^{(0)}$, it assigns to each datapoint ${x}_i$ the closest (in the sense of the Euclidean distance) centroid among ${\mu}$, noted ${\mu}({x}_i)$, and moves ${\mu}_k$ accordingly to the average of all datapoints projecting on it, namely ${\mu}_k^{(t+1)} = \mathbb{E}({x}_i \given {\mu}({x}_i) = {\mu}_k^{(t)})$. As we can see, this formulation assigns a single centroid to a datapoint without any level of uncertainty, making it a ``hard'' version of clustering. In case of overlapping clusters, such as the two on the left of Fig. \ref{fig:clustering}, it is however natural to think that datapoints living at the border could be either in the blue or the orange cluster. Also, because the association energy appearing in the above equation is based on the Euclidean distance, it tends to generate spherical groups. These restrictions of the K-means algorithm later led to the Gaussian mixture model formulation of the clustering that we will present in this chapter. This latter introduces a fuzziness by assigning a probability to each datapoint of being generated by one the $K$ clusters but also extends the definition to anisotropic clusters of various densities.
    
## Mixture models

The general form of the **normal** probability density function is:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$

{{< callout note >}}
The parameter $\mu$ is the mean or expectation of the distribution.
$\sigma$ is its standard deviation.
The variance of the distribution is $\sigma^{2}$.
{{< /callout >}}

## Quiz

{{< spoiler text="What is the parameter $\mu$?" >}}
The parameter $\mu$ is the mean or expectation of the distribution.
{{< /spoiler >}}

## EM and its drawbacks

## Practical implementation: a Python tutorial
