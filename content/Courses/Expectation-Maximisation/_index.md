---
title: Mixture models and Expectation-Maximisation algorithm
linkTitle: Mixture models and EM algorithm
summary: An overview of how the EM algorithm works with some practical implementation on the Gaussian Mixture Model.
date: '2021-11-04'
type: book
math: true
---

## Context

  Clustering is a one of the most ancient unsupervised tasks of ML aiming at identifying a partition of a given dataset into multiple groups called "clusters". The plot below illustrates this objective in a 2D case where the aim is to go from the left panel to the right one, by attributing a class to each input datapoint. 

<p align="center">
<img src="https://github.com/tbonnair/academic-online-cv/blob/master/content/Courses/Expectation-Maximisation/clustering_illustration.png?raw=true" alt="fig:clustering"/>
</p>
  
  The common ground to many methods that have been proposed these past decades is that they all embed, in some ways, a measure of the "similarity" between datapoints living inside the same cluster. The simplest way of thinking this is a measure based on the Euclidean distance leading to representations in which "close" datapoints more likely fall into the same cluster while distant ones should reside in different clusters. The prototypical method to partition a $D$-dimensional dataset into $K$ clusters is the K-means algorithm proposed by Macqueen in 1967. We can write the K-means goal as the optimisation problem aiming at finding the set of $K$ points (hereafter called centres) minimising the sum of squared distances between datapoints and cluster centres. We end up in a setting seeking to find the set of points $\left( \boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_K \right)$ minimising

$$
\sum_{i=1}^N \operatorname*{min}_{k \in \lbrace 1, \ldots, K \rbrace } \lVert \boldsymbol{x}_i - \boldsymbol{\mu}_k \rVert^2_2.
$$

 The K-means algorithm solves this optimisation problem in a simple way. Starting from a set of initial positions ${\mu}^{(0)}$, it assigns to each datapoint $\boldsymbol{x}_i$ the closest (in the sense of the Euclidean distance) centroid among $\boldsymbol{\mu}$, noted ${\mu}({x}_i)$, and moves ${\mu}_k$ accordingly to the average of all datapoints projecting on it, namely $\boldsymbol{\mu}_k^{(t+1)} = \mathbb{E}(\boldsymbol{x}_i | \boldsymbol{\mu}({x}_i) = \boldsymbol{\mu}_k^{(t)})$. As we can see, this formulation assigns a single centroid to a datapoint without any level of uncertainty, making it a "hard" version of clustering. In case of overlapping clusters, such as the two on the left of the first Figure, it is however natural to think that datapoints living at the border could be either in the blue or the orange cluster. Also, because the association energy appearing in the above equation is based on the Euclidean distance, it tends to generate spherical groups. These restrictions of the K-means algorithm later led to the **Gaussian mixture model** formulation of the clustering. This latter introduces a fuzziness by assigning a probability to each datapoint of being generated by one the $K$ clusters but also extends the definition to anisotropic clusters of various densities. Let us see how it works!
    
## Mixture models

  Real-life data often come as drawn from probability distributions with complex shapes and multiple modes that cannot be satisfactorily described by a single well-known probability distribution. Mixture models propose to model this complexity by using a linear combination of $K$ known laws. The Figure below illustrates this need on a multi-modal 2D distribution of points. Although easy to study, a single Gaussian component (the grey ellipse) do not explain fairly well the non-Gaussian dataset while a linear combination of three Gaussian (the set of coloured ellipses) is leading to a better representation.

<p align="center">
<img src="https://github.com/tbonnair/academic-online-cv/blob/master/content/Courses/Expectation-Maximisation/gaussian_mixtures.png?raw=true" alt="fig:GMM"/>
</p>

  Mathematically, a mixture distribution is the linear combination of $K$ probability distributions individually denoted $ f_k(\boldsymbol{x},\boldsymbol{\theta}_k) $ with parameters $ \boldsymbol{\theta}_k $. The probability of a datapoint $ \boldsymbol{x} $ being generated by the model is hence

$$
p(\boldsymbol{x} | \boldsymbol{\Theta}) = \sum_{k=1}^K \pi_k f_k(\boldsymbol{x}, \boldsymbol{\theta}_k), 
$$
  with $\boldsymbol{\Theta} = \lbrace \pi_1, \ldots, \pi_K, \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_K \rbrace $ the set of model parameters where $ \pi_k $ is the mixing coefficient (also called amplitude) of component $k$. Note that, given the properties of probability distributions, amplitudes are normalised and positive by definition, namely $ \sum_{k=1}^K \pi_k = 1$ and $\forall k \in \lbrace 1, \ldots, K \rbrace, \pi_k \geq 0 $.

  This idea of combination of known laws, despite its conceptual simplicity, can lead to accurate representations of highly complex density distributions! This is in particular why mixture distributions are nowadays at the basis of many mathematical tools used in machine learning like kernel density estimation, clustering or mixture density networks.

  A particular class of mixture model is the Gaussian case, where $\forall k \in \lbrace 1, \ldots, K \rbrace, f_k(\boldsymbol{x}, \boldsymbol{\theta}_k) = \mathcal{N}(\boldsymbol{x}, \boldsymbol{\theta}_k)$, with
$$
\mathcal{N}(\boldsymbol{x}, \boldsymbol{\theta}_k) = \frac{\exp{ -\frac{1}{2} \left(\boldsymbol{x} - \boldsymbol{\mu}_k\right)^\mathrm{T} \boldsymbol{\Sigma}_k^{-1} \left(\boldsymbol{x} - \boldsymbol{\mu}_k\right)}}{(2\pi)^{D/2} \lvert \boldsymbol{\Sigma}_k \rvert^{1/2}},
$$
the Gaussian probability distribution with parameter $\boldsymbol{\theta}_k = \lbrace \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \rbrace$, respectively corresponding to the average of the $k^\mathrm{th}$ Gaussian component and its covariance matrix.

## Expectation-Maximisation

For now, we have modelled our datasets with a linear combination of $K$ laws. We now need to estimate to optimal set of parameters $\boldsymbol{\Theta}$ that fit a given dataset $\boldsymbol{X} = \lbrace \boldsymbol{x}_i \rbrace $.
As usually done in statistics, we can do that by maximising the likelihood of the model. Intuitively, the likelihood encode the information of how probable it is that our observed dataset was generated by the model. Mathematically, it is written $p(\boldsymbol{X} | \boldsymbol{\Theta}) $ and is often expressed in $\log$ for convenience. Assuming independence of the datapoints and a mixture model, we can simply write the log-likelihood as the sum of all individual probabilities:
$$
 \log p( \boldsymbol{X} | \boldsymbol{\Theta}) = \sum_{i=1}^N \log \left( \sum_{k=1}^K \pi_k f_k(\boldsymbol{x}_i, \boldsymbol{\theta}_k) \right).
$$

{{< callout note >}}
The parameter $\mu$ is the mean or expectation of the distribution.
$\sigma$ is its standard deviation.
The variance of the distribution is $\sigma^{2}$.
{{< /callout >}}

{{< spoiler text="What is the parameter $\mu$?" >}}
The parameter $\mu$ is the mean or expectation of the distribution.
{{< /spoiler >}}

## Practical implementation: a Python tutorial
